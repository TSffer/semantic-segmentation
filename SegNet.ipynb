{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TSffer/semantic-segmentation/blob/master/SegNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StwD0mHrkMDA",
        "colab_type": "code",
        "outputId": "fdf4de17-a797-4961-a206-774dd1efaffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%load_ext tensorboard\n",
        "from keras.backend.tensorflow_backend import  set_session\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if(device_name != '/device:GPU:0'):\n",
        "     raise SystemError(\"GPU device not found\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.log_device_placement = True\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "set_session(sess)\n",
        "\n",
        "import keras\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "import keras.backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorboardcolab import *\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime\n",
        "import cv2\n",
        "import glob\n",
        "import itertools\n",
        "import os\n",
        "import six\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from keras.models import load_model\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from types import MethodType\n",
        "import pytest\n",
        "import tempfile\n",
        "\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Found GPU at: /device:GPU:0\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz0JlJ9fGngp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_ORDERING_CHANNELS_LAST = \"channels_last\"\n",
        "IMAGE_ORDERING_CHANNELS_FIRST = \"channels_first\"\n",
        "\n",
        "# Default IMAGE_ORDERING = channels_last\n",
        "IMAGE_ORDERING = IMAGE_ORDERING_CHANNELS_LAST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDtNqPmiF_Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if IMAGE_ORDERING == 'channels_first':\n",
        "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
        "                     \"releases/download/v0.1/\" \\\n",
        "                     \"vgg16_weights_th_dim_ordering_th_kernels_notop.h5\"\n",
        "elif IMAGE_ORDERING == 'channels_last':\n",
        "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
        "                     \"releases/download/v0.1/\" \\\n",
        "                     \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n",
        "\n",
        "def get_vgg_encoder(input_height=224,  input_width=224, pretrained='imagenet'):\n",
        "\n",
        "    assert input_height % 32 == 0\n",
        "    assert input_width % 32 == 0\n",
        "\n",
        "    if IMAGE_ORDERING == 'channels_first':\n",
        "        img_input = Input(shape=(3, input_height, input_width))\n",
        "    elif IMAGE_ORDERING == 'channels_last':\n",
        "        img_input = Input(shape=(input_height, input_width, 3))\n",
        "\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "               name='block1_conv1', data_format=IMAGE_ORDERING)(img_input)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "               name='block1_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f1 = x\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
        "               name='block2_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
        "               name='block2_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f2 = x\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
        "               name='block3_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
        "               name='block3_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
        "               name='block3_conv3', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f3 = x\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block4_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block4_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block4_conv3', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f4 = x\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block5_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block5_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block5_conv3', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f5 = x\n",
        "\n",
        "    if pretrained == 'imagenet':\n",
        "        VGG_Weights_path = tf.keras.utils.get_file(\n",
        "            pretrained_url.split(\"/\")[-1], pretrained_url)\n",
        "        Model(img_input, x).load_weights(VGG_Weights_path)\n",
        "\n",
        "    return img_input, [f1, f2, f3, f4, f5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0923wVoeJjBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import imgaug as ia\n",
        "    from imgaug import augmenters as iaa\n",
        "except ImportError:\n",
        "    print(\"Error in loading augmentation, can't import imgaug. Please make sure it is installed.\")\n",
        "\n",
        "\n",
        "IMAGE_AUGMENTATION_SEQUENCE = None\n",
        "IMAGE_AUGMENTATION_NUM_TRIES = 10\n",
        "\n",
        "def _load_augmentation():\n",
        "    \"\"\" Load image augmentation model \"\"\"\n",
        "\n",
        "    def sometimes(aug):\n",
        "        return iaa.Sometimes(0.5, aug)\n",
        "\n",
        "    global IMAGE_AUGMENTATION_SEQUENCE\n",
        "    IMAGE_AUGMENTATION_SEQUENCE = iaa.Sequential(\n",
        "        [\n",
        "            # apply the following augmenters to most images\n",
        "            iaa.Fliplr(0.5),  # horizontally flip 50% of all images\n",
        "            iaa.Flipud(0.2),  # vertically flip 20% of all images\n",
        "            # crop images by -5% to 10% of their height/width\n",
        "            sometimes(iaa.CropAndPad(\n",
        "                percent=(-0.05, 0.1),\n",
        "                pad_mode=ia.ALL,\n",
        "                pad_cval=(0, 255)\n",
        "            )),\n",
        "            sometimes(iaa.Affine(\n",
        "                # scale images to 80-120% of their size, individually per axis\n",
        "                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
        "                # translate by -20 to +20 percent (per axis)\n",
        "                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "                rotate=(-45, 45),  # rotate by -45 to +45 degrees\n",
        "                shear=(-16, 16),  # shear by -16 to +16 degrees\n",
        "                # use nearest neighbour or bilinear interpolation (fast)\n",
        "                order=[0, 1],\n",
        "                # if mode is constant, use a cval between 0 and 255\n",
        "                cval=(0, 255),\n",
        "                # use any of scikit-image's warping modes\n",
        "                # (see 2nd image from the top for examples)\n",
        "                mode=ia.ALL\n",
        "            )),\n",
        "            # execute 0 to 5 of the following (less important) augmenters per\n",
        "            # image don't execute all of them, as that would often be way too\n",
        "            # strong\n",
        "            iaa.SomeOf((0, 5),\n",
        "                       [\n",
        "                # convert images into their superpixel representation\n",
        "                sometimes(iaa.Superpixels(\n",
        "                    p_replace=(0, 1.0), n_segments=(20, 200))),\n",
        "                iaa.OneOf([\n",
        "                    # blur images with a sigma between 0 and 3.0\n",
        "                    iaa.GaussianBlur((0, 3.0)),\n",
        "                    # blur image using local means with kernel sizes\n",
        "                    # between 2 and 7\n",
        "                    iaa.AverageBlur(k=(2, 7)),\n",
        "                    # blur image using local medians with kernel sizes\n",
        "                    # between 2 and 7\n",
        "                    iaa.MedianBlur(k=(3, 11)),\n",
        "                ]),\n",
        "                iaa.Sharpen(alpha=(0, 1.0), lightness=(\n",
        "                            0.75, 1.5)),  # sharpen images\n",
        "                iaa.Emboss(alpha=(0, 1.0), strength=(\n",
        "                    0, 2.0)),  # emboss images\n",
        "                # search either for all edges or for directed edges,\n",
        "                # blend the result with the original image using a blobby mask\n",
        "                iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
        "                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
        "                    iaa.DirectedEdgeDetect(\n",
        "                        alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
        "                ])),\n",
        "                # add gaussian noise to images\n",
        "                iaa.AdditiveGaussianNoise(loc=0, scale=(\n",
        "                    0.0, 0.05*255), per_channel=0.5),\n",
        "                iaa.OneOf([\n",
        "                    # randomly remove up to 10% of the pixels\n",
        "                    iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "                    iaa.CoarseDropout((0.03, 0.15), size_percent=(\n",
        "                        0.02, 0.05), per_channel=0.2),\n",
        "                ]),\n",
        "                # invert color channels\n",
        "                iaa.Invert(0.05, per_channel=True),\n",
        "                # change brightness of images (by -10 to 10 of original value)\n",
        "                iaa.Add((-10, 10), per_channel=0.5),\n",
        "                # change hue and saturation\n",
        "                iaa.AddToHueAndSaturation((-20, 20)),\n",
        "                # either change the brightness of the whole image (sometimes\n",
        "                # per channel) or change the brightness of subareas\n",
        "                iaa.OneOf([\n",
        "                    iaa.Multiply(\n",
        "                                (0.5, 1.5), per_channel=0.5),\n",
        "                    iaa.FrequencyNoiseAlpha(\n",
        "                        exponent=(-4, 0),\n",
        "                        first=iaa.Multiply(\n",
        "                            (0.5, 1.5), per_channel=True),\n",
        "                        second=iaa.ContrastNormalization(\n",
        "                            (0.5, 2.0))\n",
        "                    )\n",
        "                ]),\n",
        "                # improve or worsen the contrast\n",
        "                iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),\n",
        "                iaa.Grayscale(alpha=(0.0, 1.0)),\n",
        "                # move pixels locally around (with random strengths)\n",
        "                sometimes(iaa.ElasticTransformation(\n",
        "                    alpha=(0.5, 3.5), sigma=0.25)),\n",
        "                # sometimes move parts of the image around\n",
        "                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))),\n",
        "                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
        "            ],\n",
        "                random_order=True\n",
        "            )\n",
        "        ],\n",
        "        random_order=True\n",
        "    )\n",
        "\n",
        "\n",
        "def _augment_seg(img, seg):\n",
        "\n",
        "    if not IMAGE_AUGMENTATION_SEQUENCE:\n",
        "        _load_augmentation()\n",
        "\n",
        "    # Create a deterministic augmentation from the random one\n",
        "    aug_det = IMAGE_AUGMENTATION_SEQUENCE.to_deterministic()\n",
        "    # Augment the input image\n",
        "    image_aug = aug_det.augment_image(img)\n",
        "\n",
        "    segmap = ia.SegmentationMapOnImage(\n",
        "        seg, nb_classes=np.max(seg) + 1, shape=img.shape)\n",
        "    segmap_aug = aug_det.augment_segmentation_maps(segmap)\n",
        "    segmap_aug = segmap_aug.get_arr_int()\n",
        "\n",
        "    return image_aug, segmap_aug\n",
        "\n",
        "\n",
        "def _try_n_times(fn, n, *args, **kargs):\n",
        "    \"\"\" Try a function N times \"\"\"\n",
        "    attempts = 0\n",
        "    while attempts < n:\n",
        "        try:\n",
        "            return fn(*args, **kargs)\n",
        "        except Exception:\n",
        "            attempts += 1\n",
        "\n",
        "    return fn(*args, **kargs)\n",
        "\n",
        "\n",
        "def augment_seg(img, seg):\n",
        "    return _try_n_times(_augment_seg, IMAGE_AUGMENTATION_NUM_TRIES, img, seg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVAbVz4aIYVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print(\"tqdm not found, disabling progress bars\")\n",
        "    def tqdm(iter):\n",
        "        return iter\n",
        "\n",
        "DATA_LOADER_SEED = 0\n",
        "\n",
        "random.seed(DATA_LOADER_SEED)\n",
        "class_colors = [(random.randint(0, 255), random.randint(\n",
        "    0, 255), random.randint(0, 255)) for _ in range(5000)]\n",
        "\n",
        "\n",
        "class DataLoaderError(Exception):\n",
        "    pass\n",
        "\n",
        "def get_pairs_from_paths(images_path, segs_path, ignore_non_matching=False):\n",
        "    \"\"\" Find all the images from the images_path directory and\n",
        "        the segmentation images from the segs_path directory\n",
        "        while checking integrity of data \"\"\"\n",
        "\n",
        "    ACCEPTABLE_IMAGE_FORMATS = [\".jpg\", \".jpeg\", \".png\"]\n",
        "    ACCEPTABLE_SEGMENTATION_FORMATS = [\".png\"]\n",
        "\n",
        "    image_files = []\n",
        "    segmentation_files = {}\n",
        "\n",
        "    for dir_entry in os.listdir(images_path):\n",
        "        if os.path.isfile(os.path.join(images_path, dir_entry)) and \\\n",
        "                os.path.splitext(dir_entry)[1] in ACCEPTABLE_IMAGE_FORMATS:\n",
        "            file_name, file_extension = os.path.splitext(dir_entry)\n",
        "            image_files.append((file_name, file_extension, os.path.join(images_path, dir_entry)))\n",
        "\n",
        "    for dir_entry in os.listdir(segs_path):\n",
        "        if os.path.isfile(os.path.join(segs_path, dir_entry)) and \\\n",
        "                os.path.splitext(dir_entry)[1] in ACCEPTABLE_SEGMENTATION_FORMATS:\n",
        "            file_name, file_extension = os.path.splitext(dir_entry)\n",
        "            if file_name in segmentation_files:\n",
        "                raise DataLoaderError(\"Segmentation file with filename {0} already exists and is ambiguous to resolve with path {1}. Please remove or rename the latter.\".format(file_name, os.path.join(segs_path, dir_entry)))\n",
        "            segmentation_files[file_name] = (file_extension, os.path.join(segs_path, dir_entry))\n",
        "\n",
        "    return_value = []\n",
        "    # Match the images and segmentations\n",
        "    for image_file, _, image_full_path in image_files:\n",
        "        if image_file in segmentation_files:\n",
        "            return_value.append((image_full_path, segmentation_files[image_file][1]))\n",
        "        elif ignore_non_matching:\n",
        "            continue\n",
        "        else:\n",
        "            # Error out\n",
        "            raise DataLoaderError(\"No corresponding segmentation found for image {0}.\".format(image_full_path))\n",
        "\n",
        "    return return_value\n",
        "\n",
        "\n",
        "def get_image_array(image_input, width, height, imgNorm=\"sub_mean\",\n",
        "                  ordering='channels_first'):\n",
        "    \"\"\" Load image array from input \"\"\"\n",
        "\n",
        "    if type(image_input) is np.ndarray:\n",
        "        # It is already an array, use it as it is\n",
        "        img = image_input\n",
        "    elif  isinstance(image_input, six.string_types)  :\n",
        "        if not os.path.isfile(image_input):\n",
        "            raise DataLoaderError(\"get_image_array: path {0} doesn't exist\".format(image_input))\n",
        "        img = cv2.imread(image_input, 1)\n",
        "    else:\n",
        "        raise DataLoaderError(\"get_image_array: Can't process input type {0}\".format(str(type(image_input))))\n",
        "\n",
        "    if imgNorm == \"sub_and_divide\":\n",
        "        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n",
        "    elif imgNorm == \"sub_mean\":\n",
        "        img = cv2.resize(img, (width, height))\n",
        "        img = img.astype(np.float32)\n",
        "        img[:, :, 0] -= 103.939\n",
        "        img[:, :, 1] -= 116.779\n",
        "        img[:, :, 2] -= 123.68\n",
        "        img = img[:, :, ::-1]\n",
        "    elif imgNorm == \"divide\":\n",
        "        img = cv2.resize(img, (width, height))\n",
        "        img = img.astype(np.float32)\n",
        "        img = img/255.0\n",
        "\n",
        "    if ordering == 'channels_first':\n",
        "        img = np.rollaxis(img, 2, 0)\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_segmentation_array(image_input, nClasses, width, height, no_reshape=False):\n",
        "    \"\"\" Load segmentation array from input \"\"\"\n",
        "\n",
        "    seg_labels = np.zeros((height, width, nClasses))\n",
        "\n",
        "    if type(image_input) is np.ndarray:\n",
        "        # It is already an array, use it as it is\n",
        "        img = image_input\n",
        "    elif isinstance(image_input, six.string_types) :\n",
        "        if not os.path.isfile(image_input):\n",
        "            raise DataLoaderError(\"get_segmentation_array: path {0} doesn't exist\".format(image_input))\n",
        "        img = cv2.imread(image_input, 1)\n",
        "    else:\n",
        "        raise DataLoaderError(\"get_segmentation_array: Can't process input type {0}\".format(str(type(image_input))))\n",
        "\n",
        "    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_NEAREST)\n",
        "    img = img[:, :, 0]\n",
        "\n",
        "    for c in range(nClasses):\n",
        "        seg_labels[:, :, c] = (img == c).astype(int)\n",
        "\n",
        "    if not no_reshape:\n",
        "        seg_labels = np.reshape(seg_labels, (width*height, nClasses))\n",
        "\n",
        "    return seg_labels\n",
        "\n",
        "\n",
        "def verify_segmentation_dataset(images_path, segs_path, n_classes, show_all_errors=False):\n",
        "    try:\n",
        "        img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n",
        "        if not len(img_seg_pairs):\n",
        "            print(\"Couldn't load any data from images_path: {0} and segmentations path: {1}\".format(images_path, segs_path))\n",
        "            return False\n",
        "\n",
        "        return_value = True\n",
        "        for im_fn, seg_fn in tqdm(img_seg_pairs):\n",
        "            img = cv2.imread(im_fn)\n",
        "            seg = cv2.imread(seg_fn)\n",
        "            # Check dimensions match\n",
        "            if not img.shape == seg.shape:\n",
        "                return_value = False\n",
        "                print(\"The size of image {0} and its segmentation {1} doesn't match (possibly the files are corrupt).\".format(im_fn, seg_fn))\n",
        "                if not show_all_errors:\n",
        "                    break\n",
        "            else:\n",
        "                max_pixel_value = np.max(seg[:, :, 0])\n",
        "                if max_pixel_value >= n_classes:\n",
        "                    return_value = False\n",
        "                    print(\"The pixel values of the segmentation image {0} violating range [0, {1}]. Found maximum pixel value {2}\".format(seg_fn, str(n_classes - 1), max_pixel_value))\n",
        "                    if not show_all_errors:\n",
        "                        break\n",
        "        if return_value:\n",
        "            print(\"Dataset verified! \")\n",
        "        else:\n",
        "            print(\"Dataset not verified!\")\n",
        "        return return_value\n",
        "    except DataLoaderError as e:\n",
        "        print(\"Found error during data loading\\n{0}\".format(str(e)))\n",
        "        return False\n",
        "\n",
        "def image_segmentation_generator(images_path, segs_path, batch_size,\n",
        "                                 n_classes, input_height, input_width,\n",
        "                                 output_height, output_width,\n",
        "                                 do_augment=False):\n",
        "\n",
        "    img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n",
        "    random.shuffle(img_seg_pairs)\n",
        "    zipped = itertools.cycle(img_seg_pairs)\n",
        "\n",
        "    while True:\n",
        "        X = []\n",
        "        Y = []\n",
        "        for _ in range(batch_size):\n",
        "            im, seg = next(zipped)\n",
        "\n",
        "            im = cv2.imread(im, 1)\n",
        "            seg = cv2.imread(seg, 1)\n",
        "\n",
        "            if do_augment:\n",
        "                im, seg[:, :, 0] = augment_seg(im, seg[:, :, 0])\n",
        "\n",
        "            X.append(get_image_array(im, input_width,\n",
        "                                   input_height, ordering=IMAGE_ORDERING))\n",
        "            Y.append(get_segmentation_array(\n",
        "                seg, n_classes, output_width, output_height))\n",
        "\n",
        "        yield np.array(X), np.array(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSj9MrtrIAz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPS = 1e-12\n",
        "\n",
        "\n",
        "def get_iou(gt, pr, n_classes):\n",
        "    class_wise = np.zeros(n_classes)\n",
        "    for cl in range(n_classes):\n",
        "        intersection = np.sum((gt == cl)*(pr == cl))\n",
        "        union = np.sum(np.maximum((gt == cl), (pr == cl)))\n",
        "        iou = float(intersection)/(union + EPS)\n",
        "        class_wise[cl] = iou\n",
        "    return class_wise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T61lK_nSIAjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(DATA_LOADER_SEED)\n",
        "\n",
        "def model_from_checkpoint_path(checkpoints_path):\n",
        "\n",
        "    assert (os.path.isfile(checkpoints_path+\"_config.json\")\n",
        "            ), \"Checkpoint not found.\"\n",
        "    model_config = json.loads(\n",
        "        open(checkpoints_path+\"_config.json\", \"r\").read())\n",
        "    latest_weights = find_latest_checkpoint(checkpoints_path)\n",
        "    assert (latest_weights is not None), \"Checkpoint not found.\"\n",
        "    model = model_from_name[model_config['model_class']](\n",
        "        model_config['n_classes'], input_height=model_config['input_height'],\n",
        "        input_width=model_config['input_width'])\n",
        "    print(\"loaded weights \", latest_weights)\n",
        "    model.load_weights(latest_weights)\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model=None, inp=None, out_fname=None, checkpoints_path=None):\n",
        "\n",
        "    if model is None and (checkpoints_path is not None):\n",
        "        model = model_from_checkpoint_path(checkpoints_path)\n",
        "    #model.summary()\n",
        "    assert (inp is not None)\n",
        "    assert((type(inp) is np.ndarray) or isinstance(inp, six.string_types)\n",
        "           ), \"Inupt should be the CV image or the input file name\"\n",
        "\n",
        "    if isinstance(inp, six.string_types):\n",
        "        inp = cv2.imread(inp)\n",
        "\n",
        "    assert len(inp.shape) == 3, \"Image should be h,w,3 \"\n",
        "    orininal_h = inp.shape[0]\n",
        "    orininal_w = inp.shape[1]\n",
        "\n",
        "    output_width = model.output_width\n",
        "    output_height = model.output_height\n",
        "    input_width = model.input_width\n",
        "    input_height = model.input_height\n",
        "    n_classes = model.n_classes\n",
        "\n",
        "    x = get_image_array(inp, input_width, input_height, ordering=IMAGE_ORDERING)\n",
        "    pr = model.predict(np.array([x]))[0]\n",
        "    pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)\n",
        "\n",
        "    seg_img = np.zeros((output_height, output_width, 3))\n",
        "    colors = class_colors\n",
        "\n",
        "    for c in range(n_classes):\n",
        "        seg_img[:, :, 0] += ((pr[:, :] == c)*(colors[c][0])).astype('uint8')\n",
        "        seg_img[:, :, 1] += ((pr[:, :] == c)*(colors[c][1])).astype('uint8')\n",
        "        seg_img[:, :, 2] += ((pr[:, :] == c)*(colors[c][2])).astype('uint8')\n",
        "\n",
        "    seg_img = cv2.resize(seg_img, (orininal_w, orininal_h))\n",
        "\n",
        "    if out_fname is not None:\n",
        "        cv2.imwrite(out_fname, seg_img)\n",
        "\n",
        "    return pr\n",
        "\n",
        "\n",
        "def predict_multiple(model=None, inps=None, inp_dir=None, out_dir=None,\n",
        "                     checkpoints_path=None):\n",
        "\n",
        "    if model is None and (checkpoints_path is not None):\n",
        "        model = model_from_checkpoint_path(checkpoints_path)\n",
        "\n",
        "    if inps is None and (inp_dir is not None):\n",
        "        inps = glob.glob(os.path.join(inp_dir, \"*.jpg\")) + glob.glob(\n",
        "            os.path.join(inp_dir, \"*.png\")) + \\\n",
        "            glob.glob(os.path.join(inp_dir, \"*.jpeg\"))\n",
        "\n",
        "    assert type(inps) is list\n",
        "\n",
        "    all_prs = []\n",
        "\n",
        "    for i, inp in enumerate(tqdm(inps)):\n",
        "        if out_dir is None:\n",
        "            out_fname = None\n",
        "        else:\n",
        "            if isinstance(inp, six.string_types):\n",
        "                out_fname = os.path.join(out_dir, os.path.basename(inp))\n",
        "            else:\n",
        "                out_fname = os.path.join(out_dir, str(i) + \".jpg\")\n",
        "\n",
        "        pr = predict(model, inp, out_fname)\n",
        "        all_prs.append(pr)\n",
        "\n",
        "    return all_prs\n",
        "\n",
        "\n",
        "\n",
        "def evaluate( model=None , inp_images=None , annotations=None,inp_images_dir=None ,annotations_dir=None , checkpoints_path=None ):\n",
        "    \n",
        "    if model is None:\n",
        "        assert (checkpoints_path is not None) , \"Please provide the model or the checkpoints_path\"\n",
        "        model = model_from_checkpoint_path(checkpoints_path)\n",
        "        \n",
        "    if inp_images is None:\n",
        "        assert (inp_images_dir is not None) , \"Please privide inp_images or inp_images_dir\"\n",
        "        assert (annotations_dir is not None) , \"Please privide inp_images or inp_images_dir\"\n",
        "        \n",
        "        paths = get_pairs_from_paths(inp_images_dir , annotations_dir )\n",
        "        paths = list(zip(*paths))\n",
        "        inp_images = list(paths[0])\n",
        "        annotations = list(paths[1])\n",
        "        \n",
        "    assert type(inp_images) is list\n",
        "    assert type(annotations) is list\n",
        "        \n",
        "    tp = np.zeros( model.n_classes  )\n",
        "    fp = np.zeros( model.n_classes  )\n",
        "    fn = np.zeros( model.n_classes  )\n",
        "    n_pixels = np.zeros( model.n_classes  )\n",
        "    \n",
        "    for inp , ann   in tqdm( zip( inp_images , annotations )):\n",
        "        pr = predict(model , inp )\n",
        "        gt = get_segmentation_array( ann , model.n_classes ,  model.output_width , model.output_height , no_reshape=True  )\n",
        "        gt = gt.argmax(-1)\n",
        "        pr = pr.flatten()\n",
        "        gt = gt.flatten()\n",
        "                \n",
        "        for cl_i in range(model.n_classes ):\n",
        "            \n",
        "            tp[ cl_i ] += np.sum( (pr == cl_i) * (gt == cl_i) )\n",
        "            fp[ cl_i ] += np.sum( (pr == cl_i) * ((gt != cl_i)) )\n",
        "            fn[ cl_i ] += np.sum( (pr != cl_i) * ((gt == cl_i)) )\n",
        "            n_pixels[ cl_i ] += np.sum( gt == cl_i  )\n",
        "            \n",
        "    cl_wise_score = tp / ( tp + fp + fn + 0.000000000001 )\n",
        "    n_pixels_norm = n_pixels /  np.sum(n_pixels)\n",
        "    frequency_weighted_IU = np.sum(cl_wise_score*n_pixels_norm)\n",
        "    mean_IU = np.mean(cl_wise_score)\n",
        "    return {\"frequency_weighted_IU\":frequency_weighted_IU , \"mean_IU\":mean_IU , \"class_wise_IU\":cl_wise_score }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paRPBCs4eaop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_latest_checkpoint(checkpoints_path, fail_safe=True):\n",
        "\n",
        "    def get_epoch_number_from_path(path):\n",
        "        return path.replace(checkpoints_path, \"\").strip(\".\")\n",
        "\n",
        "    # Get all matching files\n",
        "    all_checkpoint_files = glob.glob(checkpoints_path + \".*\")\n",
        "    # Filter out entries where the epoc_number part is pure number\n",
        "    all_checkpoint_files = list(filter(lambda f: get_epoch_number_from_path(f).isdigit(), all_checkpoint_files))\n",
        "    if not len(all_checkpoint_files):\n",
        "        # The glob list is empty, don't have a checkpoints_path\n",
        "        if not fail_safe:\n",
        "            raise ValueError(\"Checkpoint path {0} invalid\".format(checkpoints_path))\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Find the checkpoint file with the maximum epoch\n",
        "    latest_epoch_checkpoint = max(all_checkpoint_files, key=lambda f: int(get_epoch_number_from_path(f)))\n",
        "    return latest_epoch_checkpoint\n",
        "\n",
        "\n",
        "def train(model,\n",
        "          train_images,\n",
        "          train_annotations,\n",
        "          input_height=None,\n",
        "          input_width=None,\n",
        "          n_classes=None,\n",
        "          verify_dataset=True,\n",
        "          checkpoints_path=None,\n",
        "          epochs=5,\n",
        "          batch_size=2,\n",
        "          validate=False,\n",
        "          val_images=None,\n",
        "          val_annotations=None,\n",
        "          val_batch_size=2,\n",
        "          validation_steps=2,\n",
        "          auto_resume_checkpoint=False,\n",
        "          load_weights=None,\n",
        "          steps_per_epoch=512,\n",
        "          optimizer_name='adam'\n",
        "          ):\n",
        "    \n",
        "    result=None\n",
        "    tb = TensorBoard(log_dir='logs',write_graph=True)\n",
        "    mc = ModelCheckpoint(mode='max',filepath='/content/drive/My Drive/checkpointsSegNet/segnet_vgg.1', monitor='acc',save_best_only='True',save_weights_only='True',verbose=1)\n",
        "    es = EarlyStopping(mode='min',monitor='val_loss',patience=70,verbose=1)\n",
        "    callbacks = [tb,mc,es]\n",
        "    \n",
        "    # check if user gives model name instead of the model object\n",
        "    if isinstance(model, six.string_types):\n",
        "        # create the model from the name\n",
        "        assert (n_classes is not None), \"Please provide the n_classes\"\n",
        "        if (input_height is not None) and (input_width is not None):\n",
        "            model = model_from_name[model](\n",
        "                n_classes, input_height=input_height, input_width=input_width)\n",
        "        else:\n",
        "            model = model_from_name[model](n_classes)\n",
        "\n",
        "    n_classes = model.n_classes\n",
        "    input_height = model.input_height\n",
        "    input_width = model.input_width\n",
        "    output_height = model.output_height\n",
        "    output_width = model.output_width\n",
        "\n",
        "    if validate:\n",
        "        assert val_images is not None\n",
        "        assert val_annotations is not None\n",
        "\n",
        "    if optimizer_name is not None:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizer_name,\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    if checkpoints_path is not None:\n",
        "        with open(checkpoints_path+\"_config.json\", \"w\") as f:\n",
        "            json.dump({\n",
        "                \"model_class\": model.model_name,\n",
        "                \"n_classes\": n_classes,\n",
        "                \"input_height\": input_height,\n",
        "                \"input_width\": input_width,\n",
        "                \"output_height\": output_height,\n",
        "                \"output_width\": output_width\n",
        "            }, f)\n",
        "\n",
        "    if load_weights is not None and len(load_weights) > 0:\n",
        "        print(\"Loading weights from \", load_weights)\n",
        "        model.load_weights(load_weights)\n",
        "\n",
        "    if auto_resume_checkpoint and (checkpoints_path is not None):\n",
        "        latest_checkpoint = find_latest_checkpoint(checkpoints_path)\n",
        "        if latest_checkpoint is not None:\n",
        "            print(\"Loading the weights from latest checkpoint \",\n",
        "                  latest_checkpoint)\n",
        "            model.load_weights(latest_checkpoint)\n",
        "\n",
        "    if verify_dataset:\n",
        "        print(\"Verifying training dataset\")\n",
        "        verified = verify_segmentation_dataset(train_images, train_annotations, n_classes)\n",
        "        assert verified\n",
        "        if validate:\n",
        "            print(\"Verifying validation dataset\")\n",
        "            verified = verify_segmentation_dataset(val_images, val_annotations, n_classes)\n",
        "            assert verified\n",
        "\n",
        "    train_gen = image_segmentation_generator(\n",
        "        train_images, train_annotations,  batch_size,  n_classes,\n",
        "        input_height, input_width, output_height, output_width)\n",
        "\n",
        "    if validate:\n",
        "        val_gen = image_segmentation_generator(\n",
        "            val_images, val_annotations,  val_batch_size,\n",
        "            n_classes, input_height, input_width, output_height, output_width)\n",
        "\n",
        "    if not validate:\n",
        "        #for ep in range(epochs):\n",
        "            #print(\"Starting Epoch \", ep)\n",
        "        result=model.fit_generator(train_gen, steps_per_epoch, epochs=epochs,callbacks=callbacks,use_multiprocessing=True,verbose=1)\n",
        "            #if checkpoints_path is not None:\n",
        "        model.save_weights(checkpoints_path + \".\" + '1',overwrite=True)\n",
        "                #print(\"saved \", checkpoints_path + \".model.\" + str(ep))\n",
        "            #print(\"Finished Epoch\", ep)\n",
        "    else:\n",
        "        #for ep in range(epochs):\n",
        "            #print(\"Starting Epoch \", ep)\n",
        "        result=model.fit_generator(train_gen, steps_per_epoch,\n",
        "                                   validation_data=val_gen,\n",
        "                                   validation_steps=validation_steps,  epochs=epochs,callbacks=callbacks,use_multiprocessing=True,verbose=1)\n",
        "            #if checkpoints_path is not None:\n",
        "        model.save_weights(checkpoints_path + \".\" + '1',overwrite=True)\n",
        "                #print(\"saved \", checkpoints_path + \".model.\" + str(ep))\n",
        "            #print(\"Finished Epoch\", ep)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_0IBqHeH2Fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source m1 , dest m2\n",
        "def transfer_weights(m1, m2, verbose=True):\n",
        "\n",
        "    assert len(m1.layers) == len(\n",
        "        m2.layers), \"Both models should have same number of layers\"\n",
        "\n",
        "    nSet = 0\n",
        "    nNotSet = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Copying weights \")\n",
        "        bar = tqdm(zip(m1.layers, m2.layers))\n",
        "    else:\n",
        "        bar = zip(m1.layers, m2.layers)\n",
        "\n",
        "    for l, ll in bar:\n",
        "\n",
        "        if not any([w.shape != ww.shape for w, ww in zip(list(l.weights),\n",
        "                                                         list(ll.weights))]):\n",
        "            if len(list(l.weights)) > 0:\n",
        "                ll.set_weights(l.get_weights())\n",
        "                nSet += 1\n",
        "        else:\n",
        "            nNotSet += 1\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Copied weights of %d layers and skipped %d layers\" %\n",
        "              (nSet, nNotSet))\n",
        "\n",
        "\n",
        "def resize_image(inp,  s, data_format):\n",
        "\n",
        "    try:\n",
        "\n",
        "        return Lambda(lambda x: K.resize_images(x,\n",
        "                                                height_factor=s[0],\n",
        "                                                width_factor=s[1],\n",
        "                                                data_format=data_format,\n",
        "                                                interpolation='bilinear'))(inp)\n",
        "\n",
        "    except Exception as e:\n",
        "        # if keras is old, then rely on the tf function\n",
        "        # Sorry theano/cntk users!!!\n",
        "        assert data_format == 'channels_last'\n",
        "        assert IMAGE_ORDERING == 'channels_last'\n",
        "\n",
        "        import tensorflow as tf\n",
        "\n",
        "        return Lambda(\n",
        "            lambda x: tf.image.resize_images(\n",
        "                x, (K.int_shape(x)[1]*s[0], K.int_shape(x)[2]*s[1]))\n",
        "        )(inp)\n",
        "\n",
        "\n",
        "def get_segmentation_model(input, output):\n",
        "\n",
        "    img_input = input\n",
        "    o = output\n",
        "\n",
        "    o_shape = Model(img_input, o).output_shape\n",
        "    i_shape = Model(img_input, o).input_shape\n",
        "\n",
        "    if IMAGE_ORDERING == 'channels_first':\n",
        "        output_height = o_shape[2]\n",
        "        output_width = o_shape[3]\n",
        "        input_height = i_shape[2]\n",
        "        input_width = i_shape[3]\n",
        "        n_classes = o_shape[1]\n",
        "        o = (Reshape((-1, output_height*output_width)))(o)\n",
        "        o = (Permute((2, 1)))(o)\n",
        "    elif IMAGE_ORDERING == 'channels_last':\n",
        "        output_height = o_shape[1]\n",
        "        output_width = o_shape[2]\n",
        "        input_height = i_shape[1]\n",
        "        input_width = i_shape[2]\n",
        "        n_classes = o_shape[3]\n",
        "        o = (Reshape((output_height*output_width, -1)))(o)\n",
        "\n",
        "    o = (Activation('softmax'))(o)\n",
        "    model = Model(img_input, o)\n",
        "    model.output_width = output_width\n",
        "    model.output_height = output_height\n",
        "    model.n_classes = n_classes\n",
        "    model.input_height = input_height\n",
        "    model.input_width = input_width\n",
        "    model.model_name = \"\"\n",
        "\n",
        "    model.train = MethodType(train, model)\n",
        "    model.predict_segmentation = MethodType(predict, model)\n",
        "    model.predict_multiple = MethodType(predict_multiple, model)\n",
        "    model.evaluate_segmentation = MethodType(evaluate, model)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZLcrkEyHPZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def segnet_decoder(f, n_classes, n_up=3):\n",
        "\n",
        "    assert n_up >= 2\n",
        "\n",
        "    o = f\n",
        "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (Conv2D(512, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n",
        "    o = (BatchNormalization())(o)\n",
        "\n",
        "    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (Conv2D(256, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n",
        "    o = (BatchNormalization())(o)\n",
        "\n",
        "    for _ in range(n_up-2):\n",
        "        o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
        "        o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
        "        o = (Conv2D(128, (3, 3), padding='valid',\n",
        "             data_format=IMAGE_ORDERING))(o)\n",
        "        o = (BatchNormalization())(o)\n",
        "\n",
        "    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (Conv2D(64, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n",
        "    o = (BatchNormalization())(o)\n",
        "\n",
        "    o = Conv2D(n_classes, (3, 3), padding='same',\n",
        "               data_format=IMAGE_ORDERING)(o)\n",
        "\n",
        "    return o\n",
        "\n",
        "\n",
        "def _segnet(n_classes, encoder,  input_height=416, input_width=608,\n",
        "            encoder_level=3):\n",
        "\n",
        "    img_input, levels = encoder(\n",
        "        input_height=input_height,  input_width=input_width)\n",
        "\n",
        "    feat = levels[encoder_level]\n",
        "    o = segnet_decoder(feat, n_classes, n_up=3)\n",
        "    model = get_segmentation_model(img_input, o)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def vgg_segnet(n_classes, input_height=416, input_width=608, encoder_level=3):\n",
        "\n",
        "    model = _segnet(n_classes, get_vgg_encoder,  input_height=input_height,\n",
        "                    input_width=input_width, encoder_level=encoder_level)\n",
        "    model.model_name = \"vgg_segnet\"\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    m = vgg_segnet(101)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpST28vjTZgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_from_name = {}\n",
        "model_from_name[\"vgg_segnet\"] = vgg_segnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf0hU-nrrfWH",
        "colab_type": "code",
        "outputId": "20054995-fa6e-45e2-865e-1b0c6421ff71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSFNKn8xXpJ9",
        "colab_type": "code",
        "outputId": "9e2bd352-2712-42ab-ec2d-7f4fffd863a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tr_im = \"/content/drive/My Drive/CamVid/train\"\n",
        "tr_an = \"/content/drive/My Drive/CamVid/trainannot\"\n",
        "val_im = \"/content/drive/My Drive/CamVid/val\"\n",
        "val_an = \"/content/drive/My Drive/CamVid/valannot\"\n",
        "check_p = \"/content/drive/My Drive/checkpointsSegNet/segnet_vgg\"\n",
        "\n",
        "load_w = \"/content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\"\n",
        "\n",
        "model = vgg_segnet(n_classes=12, input_height=352, input_width=480)\n",
        "\n",
        "batch_size = 16\n",
        "num_data = 8192\n",
        "steps_per_epoch = np.ceil(float(num_data - round(0.2*num_data))/float(batch_size))\n",
        "print(steps_per_epoch)\n",
        "\n",
        "validation_steps = np.ceil(float((round(0.2*num_data)))/float(batch_size))\n",
        "print(validation_steps)\n",
        "\n",
        "num_epochs = 300\n",
        "\n",
        "result = model.train(train_images=tr_im,\n",
        "            train_annotations=tr_an,\n",
        "            checkpoints_path=check_p,\n",
        "            epochs=num_epochs,\n",
        "            batch_size=batch_size,\n",
        "            validate=True,\n",
        "            val_images=val_im,\n",
        "            val_annotations=val_an,\n",
        "            val_batch_size=batch_size,\n",
        "            #auto_resume_checkpoint=True,\n",
        "            #load_weights=load_w,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            validation_steps=validation_steps,\n",
        "            optimizer_name='adam'\n",
        "            )\n",
        "\n",
        "N = len(result.history['loss'])\n",
        "print(N)\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "\n",
        "fig.add_subplot(1,2,1)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.plot(np.arange(0, N), result.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), result.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.plot(np.arange(0, N), result.history[\"acc\"], label=\"train_accuracy\")\n",
        "plt.plot(np.arange(0, N), result.history[\"val_acc\"],label=\"val_accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n",
        "\n",
        "out = model.predict_segmentation(\n",
        "    inp=\"/content/drive/My Drive/dataset1/images_prepped_test/0016E5_07965.png\",\n",
        "    out_fname=\"/content/drive/My Drive/tmpSegNet/out.png\"\n",
        ")\n",
        "\n",
        "\n",
        "plt.imshow(out)\n",
        "\n",
        "print(model.evaluate_segmentation(inp_images_dir=\"/content/drive/My Drive/dataset1/images_prepped_test\",annotations_dir=\"/content/drive/My Drive/dataset1/annotations_prepped_test\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/367 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "410.0\n",
            "103.0\n",
            "Verifying training dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 367/367 [00:03<00:00, 107.93it/s]\n",
            " 12%|        | 12/101 [00:00<00:00, 110.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset verified! \n",
            "Verifying validation dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 101/101 [00:00<00:00, 109.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset verified! \n",
            "Epoch 1/300\n",
            "410/410 [==============================] - 312s 761ms/step - loss: 0.5664 - acc: 0.8284 - val_loss: 1.3558 - val_acc: 0.5123\n",
            "\n",
            "Epoch 00001: acc improved from -inf to 0.82840, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 2/300\n",
            "410/410 [==============================] - 300s 731ms/step - loss: 0.2876 - acc: 0.9061 - val_loss: 0.7069 - val_acc: 0.8492\n",
            "\n",
            "Epoch 00002: acc improved from 0.82840 to 0.90610, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 3/300\n",
            "410/410 [==============================] - 304s 742ms/step - loss: 0.1985 - acc: 0.9315 - val_loss: 0.3976 - val_acc: 0.8772\n",
            "\n",
            "Epoch 00003: acc improved from 0.90610 to 0.93154, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 4/300\n",
            "410/410 [==============================] - 308s 750ms/step - loss: 0.1524 - acc: 0.9453 - val_loss: 0.3754 - val_acc: 0.8904\n",
            "\n",
            "Epoch 00004: acc improved from 0.93154 to 0.94526, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 5/300\n",
            "410/410 [==============================] - 302s 737ms/step - loss: 0.1319 - acc: 0.9517 - val_loss: 0.4905 - val_acc: 0.8799\n",
            "\n",
            "Epoch 00005: acc improved from 0.94526 to 0.95167, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 6/300\n",
            "410/410 [==============================] - 305s 745ms/step - loss: 0.1147 - acc: 0.9573 - val_loss: 0.4716 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00006: acc improved from 0.95167 to 0.95729, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 7/300\n",
            "410/410 [==============================] - 307s 749ms/step - loss: 0.1043 - acc: 0.9610 - val_loss: 0.4050 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00007: acc improved from 0.95729 to 0.96099, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 8/300\n",
            "410/410 [==============================] - 302s 737ms/step - loss: 0.0950 - acc: 0.9640 - val_loss: 0.3871 - val_acc: 0.8996\n",
            "\n",
            "Epoch 00008: acc improved from 0.96099 to 0.96403, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 9/300\n",
            "410/410 [==============================] - 303s 738ms/step - loss: 0.0901 - acc: 0.9658 - val_loss: 0.6802 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00009: acc improved from 0.96403 to 0.96577, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 10/300\n",
            "410/410 [==============================] - 299s 729ms/step - loss: 0.0846 - acc: 0.9678 - val_loss: 0.4768 - val_acc: 0.8915\n",
            "\n",
            "Epoch 00010: acc improved from 0.96577 to 0.96780, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 11/300\n",
            "410/410 [==============================] - 298s 726ms/step - loss: 0.0810 - acc: 0.9690 - val_loss: 0.4330 - val_acc: 0.9049\n",
            "\n",
            "Epoch 00011: acc improved from 0.96780 to 0.96899, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 12/300\n",
            "410/410 [==============================] - 300s 733ms/step - loss: 0.0771 - acc: 0.9704 - val_loss: 0.4457 - val_acc: 0.9015\n",
            "\n",
            "Epoch 00012: acc improved from 0.96899 to 0.97043, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 13/300\n",
            "410/410 [==============================] - 302s 738ms/step - loss: 0.0738 - acc: 0.9716 - val_loss: 0.4449 - val_acc: 0.9007\n",
            "\n",
            "Epoch 00013: acc improved from 0.97043 to 0.97159, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 14/300\n",
            "410/410 [==============================] - 297s 724ms/step - loss: 0.1716 - acc: 0.9451 - val_loss: 1.1248 - val_acc: 0.7277\n",
            "\n",
            "Epoch 00014: acc did not improve from 0.97159\n",
            "Epoch 15/300\n",
            "410/410 [==============================] - 303s 738ms/step - loss: 0.1963 - acc: 0.9331 - val_loss: 1.2847 - val_acc: 0.7229\n",
            "\n",
            "Epoch 00015: acc did not improve from 0.97159\n",
            "Epoch 16/300\n",
            "410/410 [==============================] - 298s 728ms/step - loss: 0.0913 - acc: 0.9658 - val_loss: 0.4513 - val_acc: 0.8829\n",
            "\n",
            "Epoch 00016: acc did not improve from 0.97159\n",
            "Epoch 17/300\n",
            "410/410 [==============================] - 300s 732ms/step - loss: 0.0741 - acc: 0.9717 - val_loss: 0.3810 - val_acc: 0.9058\n",
            "\n",
            "Epoch 00017: acc improved from 0.97159 to 0.97173, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 18/300\n",
            "410/410 [==============================] - 302s 736ms/step - loss: 0.0646 - acc: 0.9751 - val_loss: 0.5140 - val_acc: 0.8878\n",
            "\n",
            "Epoch 00018: acc improved from 0.97173 to 0.97509, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 19/300\n",
            "410/410 [==============================] - 301s 733ms/step - loss: 0.0650 - acc: 0.9750 - val_loss: 0.5476 - val_acc: 0.8844\n",
            "\n",
            "Epoch 00019: acc did not improve from 0.97509\n",
            "Epoch 20/300\n",
            "410/410 [==============================] - 301s 733ms/step - loss: 0.0612 - acc: 0.9763 - val_loss: 0.5515 - val_acc: 0.8887\n",
            "\n",
            "Epoch 00020: acc improved from 0.97509 to 0.97635, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 21/300\n",
            "410/410 [==============================] - 301s 735ms/step - loss: 0.0604 - acc: 0.9766 - val_loss: 0.4616 - val_acc: 0.9099\n",
            "\n",
            "Epoch 00021: acc improved from 0.97635 to 0.97664, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 22/300\n",
            "410/410 [==============================] - 299s 729ms/step - loss: 0.0571 - acc: 0.9778 - val_loss: 0.5074 - val_acc: 0.8967\n",
            "\n",
            "Epoch 00022: acc improved from 0.97664 to 0.97784, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 23/300\n",
            "410/410 [==============================] - 300s 732ms/step - loss: 0.0574 - acc: 0.9777 - val_loss: 0.5489 - val_acc: 0.8853\n",
            "\n",
            "Epoch 00023: acc did not improve from 0.97784\n",
            "Epoch 24/300\n",
            "410/410 [==============================] - 305s 744ms/step - loss: 0.0593 - acc: 0.9771 - val_loss: 0.5932 - val_acc: 0.8822\n",
            "\n",
            "Epoch 00024: acc did not improve from 0.97784\n",
            "Epoch 25/300\n",
            "410/410 [==============================] - 307s 750ms/step - loss: 0.0521 - acc: 0.9797 - val_loss: 0.5856 - val_acc: 0.8880\n",
            "\n",
            "Epoch 00025: acc improved from 0.97784 to 0.97970, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 26/300\n",
            "410/410 [==============================] - 298s 727ms/step - loss: 0.0498 - acc: 0.9806 - val_loss: 0.5678 - val_acc: 0.8827\n",
            "\n",
            "Epoch 00026: acc improved from 0.97970 to 0.98060, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 27/300\n",
            "410/410 [==============================] - 303s 740ms/step - loss: 0.0491 - acc: 0.9808 - val_loss: 0.5325 - val_acc: 0.8873\n",
            "\n",
            "Epoch 00027: acc improved from 0.98060 to 0.98082, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 28/300\n",
            "410/410 [==============================] - 301s 735ms/step - loss: 0.0468 - acc: 0.9817 - val_loss: 0.4857 - val_acc: 0.8908\n",
            "\n",
            "Epoch 00028: acc improved from 0.98082 to 0.98167, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 29/300\n",
            "410/410 [==============================] - 301s 734ms/step - loss: 0.0469 - acc: 0.9816 - val_loss: 0.5702 - val_acc: 0.8996\n",
            "\n",
            "Epoch 00029: acc did not improve from 0.98167\n",
            "Epoch 30/300\n",
            "410/410 [==============================] - 306s 747ms/step - loss: 0.0438 - acc: 0.9828 - val_loss: 0.6573 - val_acc: 0.8872\n",
            "\n",
            "Epoch 00030: acc improved from 0.98167 to 0.98277, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 31/300\n",
            "410/410 [==============================] - 305s 743ms/step - loss: 0.0431 - acc: 0.9830 - val_loss: 0.5358 - val_acc: 0.8945\n",
            "\n",
            "Epoch 00031: acc improved from 0.98277 to 0.98304, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 32/300\n",
            "410/410 [==============================] - 306s 746ms/step - loss: 0.0396 - acc: 0.9844 - val_loss: 0.5536 - val_acc: 0.8973\n",
            "\n",
            "Epoch 00032: acc improved from 0.98304 to 0.98436, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 33/300\n",
            "410/410 [==============================] - 298s 727ms/step - loss: 0.0381 - acc: 0.9849 - val_loss: 0.5348 - val_acc: 0.8993\n",
            "\n",
            "Epoch 00033: acc improved from 0.98436 to 0.98490, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 34/300\n",
            "410/410 [==============================] - 304s 741ms/step - loss: 0.0394 - acc: 0.9844 - val_loss: 0.5485 - val_acc: 0.8925\n",
            "\n",
            "Epoch 00034: acc did not improve from 0.98490\n",
            "Epoch 35/300\n",
            "410/410 [==============================] - 306s 747ms/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.6497 - val_acc: 0.8899\n",
            "\n",
            "Epoch 00035: acc improved from 0.98490 to 0.98497, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 36/300\n",
            "410/410 [==============================] - 303s 739ms/step - loss: 0.0354 - acc: 0.9860 - val_loss: 0.5964 - val_acc: 0.8930\n",
            "\n",
            "Epoch 00036: acc improved from 0.98497 to 0.98596, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 37/300\n",
            "410/410 [==============================] - 299s 728ms/step - loss: 0.0358 - acc: 0.9858 - val_loss: 0.5830 - val_acc: 0.8988\n",
            "\n",
            "Epoch 00037: acc did not improve from 0.98596\n",
            "Epoch 38/300\n",
            "410/410 [==============================] - 305s 744ms/step - loss: 0.0349 - acc: 0.9862 - val_loss: 0.5324 - val_acc: 0.8972\n",
            "\n",
            "Epoch 00038: acc improved from 0.98596 to 0.98621, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 39/300\n",
            "410/410 [==============================] - 300s 732ms/step - loss: 0.0333 - acc: 0.9868 - val_loss: 0.5648 - val_acc: 0.8902\n",
            "\n",
            "Epoch 00039: acc improved from 0.98621 to 0.98677, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 40/300\n",
            "410/410 [==============================] - 302s 737ms/step - loss: 0.0304 - acc: 0.9879 - val_loss: 0.5656 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00040: acc improved from 0.98677 to 0.98789, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 41/300\n",
            "410/410 [==============================] - 301s 735ms/step - loss: 0.0298 - acc: 0.9881 - val_loss: 0.7894 - val_acc: 0.8824\n",
            "\n",
            "Epoch 00041: acc improved from 0.98789 to 0.98811, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 42/300\n",
            "410/410 [==============================] - 296s 721ms/step - loss: 0.0290 - acc: 0.9884 - val_loss: 0.8851 - val_acc: 0.8602\n",
            "\n",
            "Epoch 00042: acc improved from 0.98811 to 0.98839, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 43/300\n",
            "410/410 [==============================] - 301s 734ms/step - loss: 0.0276 - acc: 0.9890 - val_loss: 0.6603 - val_acc: 0.8934\n",
            "\n",
            "Epoch 00043: acc improved from 0.98839 to 0.98897, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 44/300\n",
            "410/410 [==============================] - 301s 734ms/step - loss: 0.0272 - acc: 0.9891 - val_loss: 0.6700 - val_acc: 0.8918\n",
            "\n",
            "Epoch 00044: acc improved from 0.98897 to 0.98911, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 45/300\n",
            "410/410 [==============================] - 300s 731ms/step - loss: 0.0246 - acc: 0.9902 - val_loss: 0.6100 - val_acc: 0.8885\n",
            "\n",
            "Epoch 00045: acc improved from 0.98911 to 0.99017, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 46/300\n",
            "410/410 [==============================] - 304s 741ms/step - loss: 0.0244 - acc: 0.9902 - val_loss: 0.6600 - val_acc: 0.8954\n",
            "\n",
            "Epoch 00046: acc improved from 0.99017 to 0.99025, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 47/300\n",
            "410/410 [==============================] - 300s 731ms/step - loss: 0.0237 - acc: 0.9905 - val_loss: 0.6741 - val_acc: 0.8910\n",
            "\n",
            "Epoch 00047: acc improved from 0.99025 to 0.99051, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 48/300\n",
            "410/410 [==============================] - 300s 732ms/step - loss: 0.0213 - acc: 0.9915 - val_loss: 0.7183 - val_acc: 0.8922\n",
            "\n",
            "Epoch 00048: acc improved from 0.99051 to 0.99146, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 49/300\n",
            "410/410 [==============================] - 296s 723ms/step - loss: 0.0209 - acc: 0.9916 - val_loss: 0.7083 - val_acc: 0.8930\n",
            "\n",
            "Epoch 00049: acc improved from 0.99146 to 0.99162, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 50/300\n",
            "410/410 [==============================] - 302s 736ms/step - loss: 0.0211 - acc: 0.9915 - val_loss: 0.6460 - val_acc: 0.8958\n",
            "\n",
            "Epoch 00050: acc did not improve from 0.99162\n",
            "Epoch 51/300\n",
            "410/410 [==============================] - 303s 740ms/step - loss: 0.0177 - acc: 0.9929 - val_loss: 0.7167 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00051: acc improved from 0.99162 to 0.99289, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 52/300\n",
            "410/410 [==============================] - 303s 738ms/step - loss: 0.0168 - acc: 0.9933 - val_loss: 0.7116 - val_acc: 0.8862\n",
            "\n",
            "Epoch 00052: acc improved from 0.99289 to 0.99327, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 53/300\n",
            "410/410 [==============================] - 297s 724ms/step - loss: 0.0177 - acc: 0.9929 - val_loss: 0.7109 - val_acc: 0.9004\n",
            "\n",
            "Epoch 00053: acc did not improve from 0.99327\n",
            "Epoch 54/300\n",
            "410/410 [==============================] - 301s 733ms/step - loss: 0.0162 - acc: 0.9936 - val_loss: 0.7316 - val_acc: 0.8924\n",
            "\n",
            "Epoch 00054: acc improved from 0.99327 to 0.99355, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 55/300\n",
            "410/410 [==============================] - 296s 721ms/step - loss: 0.0149 - acc: 0.9941 - val_loss: 0.8615 - val_acc: 0.8798\n",
            "\n",
            "Epoch 00055: acc improved from 0.99355 to 0.99411, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 56/300\n",
            "410/410 [==============================] - 296s 723ms/step - loss: 0.0116 - acc: 0.9954 - val_loss: 0.9187 - val_acc: 0.8785\n",
            "\n",
            "Epoch 00056: acc improved from 0.99411 to 0.99542, saving model to /content/drive/My Drive/checkpointsSegNet/segnet_vgg.1\n",
            "Epoch 57/300\n",
            "410/410 [==============================] - 296s 721ms/step - loss: 0.0146 - acc: 0.9943 - val_loss: 0.7878 - val_acc: 0.8880\n",
            "\n",
            "Epoch 00057: acc did not improve from 0.99542\n",
            "Epoch 58/300\n",
            "409/410 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9951"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W-IdnW4VHnXC",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2ZFeNIb3yXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoints_path=\"/content/drive/My Drive/checkpointsSegNet/segnet_vgg\"\n",
        "input_path=\"/content/drive/My Drive/CamVid/test/0001TP_008670.png\"\n",
        "output_path=\"/content/drive/My Drive/tmpSegNet/out2.png\"\n",
        "\n",
        "model_config = {\"model_class\": \"vgg_segnet\", \"n_classes\": 12, \"input_height\": 416, \"input_width\": 608, \"output_height\": 208, \"output_width\": 304}\n",
        "predict(inp=input_path, out_fname=output_path,checkpoints_path=checkpoints_path)\n",
        "predict_multiple(inp_dir=\"/content/drive/My Drive/CamVid/test\", out_dir=\"/content/drive/My Drive/tmpSegNet\",checkpoints_path=checkpoints_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0b5dPj82a_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(inp_images_dir=\"/content/drive/My Drive/dataset1/images_prepped_test\", annotations_dir=\"/content/drive/My Drive/dataset1/annotations_prepped_test\", checkpoints_path=checkpoints_path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}